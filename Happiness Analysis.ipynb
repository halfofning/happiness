{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# World Happiness Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context & Content\n",
    "The happiness scores and rankings are based on answers to the main life evaluation question asked in the poll. This question asks respondents to think of a ladder with the *best possible life for them being a 10* and the *worst possible life being a 0* and to rate their own current lives on that scale.\n",
    "\n",
    "The columns following the happiness score estimate the extent to which each of **six factors – economic production, social support, life expectancy, freedom, absence of corruption, and generosity** – contribute to making life evaluations higher in each country than they are in Dystopia, a hypothetical country that has values equal to the world’s lowest national averages for each of the six factors. They have no impact on the total score reported for each country, but they do explain why some countries rank higher than others.\n",
    "\n",
    "## Problem Statement\n",
    "Knowing from the sample dataset (2016) listing the happiness scores of representatives in each country and the factors that contribute to these scores, how accurate can we predict the happiness scores of each country in 2017?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for data analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Importing libraries for visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "# 2016\n",
    "dataset2016 = pd.read_csv('dataset/2016.csv')\n",
    "X_2016 = dataset2016.iloc[:, :-1].values\n",
    "y_2016 = dataset2016.iloc[:, 8].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "X_2016[:, 0] = labelencoder.fit_transform(X_2016[:, 0])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X_2016 = onehotencoder.fit_transform(X_2016).toarray()\n",
    "\n",
    "# Avoiding the Dummy Variable Trap\n",
    "X_2016 = X_2016[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_2016_train, X_2016_test, y_2016_train, y_2016_test = train_test_split(X_2016, y_2016, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of 3 different methods to create regression models, to find out the most accurate model to act as the final model for the prediction of happiness score for the year 2017.\n",
    "\n",
    "#### Regression Models\n",
    "- Multiple Linear Regression\n",
    "- Decision Tree\n",
    "- Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esthe\\Anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1510: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n",
      "C:\\Users\\esthe\\Anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1549: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "C:\\Users\\esthe\\Anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1550: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  * (1 - self.rsquared))\n",
      "C:\\Users\\esthe\\Anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1558: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return self.ssr/self.df_resid\n"
     ]
    }
   ],
   "source": [
    "# MODEL 1: Making use of Multiple Linear Regression\n",
    "#Fitting Multiple Linear Regression to the Training set \n",
    "from sklearn.linear_model import LinearRegression\n",
    "ml_regressor = LinearRegression()\n",
    "ml_regressor.fit(X_2016_train, y_2016_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_2016_pred_mlr = ml_regressor.predict(X_2016_test)\n",
    "\n",
    "# Building the optimal model using Backward Elimination\n",
    "#import statsmodels.formula.api as sm\n",
    "#X_2016 = np.append(arr = np.ones((155, 1)).astype(int), values = X_2016, axis = 1)\n",
    "#print(X_2016)\n",
    "#X_opt = X_2016[:,:]\n",
    "\n",
    "#Step 2\n",
    "#ml_regressor_OLS = sm.OLS(endog = y_2016, exog = X_opt).fit()\n",
    "\n",
    "#Step 3 \n",
    "#X_opt = X_2016[:,:] #removed state\n",
    "#ml_regressor_OLS = sm.OLS(endog = y_2016, exog = X_opt).fit()\n",
    "#ml_regressor_OLS.summary()''\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "def backwardElimination(x, sl):\n",
    "    numVars = len(x[0])\n",
    "    for i in range(0, numVars):\n",
    "        regressor_OLS = sm.OLS(y_2016, x).fit()\n",
    "        maxVar = max(regressor_OLS.pvalues).astype(float)\n",
    "        if maxVar > sl:\n",
    "            for j in range(0, numVars - i):\n",
    "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                    x = np.delete(x, j, 1)\n",
    "    regressor_OLS.summary()\n",
    "    return x\n",
    " \n",
    "SL = 0.05\n",
    "X_opt = X_2016[:,:]\n",
    "X_Modeled = backwardElimination(X_opt, SL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression**: Compute the root mean squared error (% accuracy) between the predicted value and the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MODEL 1: Making use of Multiple Linear Regression (RMSE)\n",
    "# rms = sqrt(mean_squared_error(y_actual, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MODEL 2: Making use of Decision Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree**: Compute the root mean squared error (% accuracy) between the predicted value and the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MODEL 2: Making use of Decision Trees (RMSE)\n",
    "# rms = sqrt(mean_squared_error(y_actual, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MODEL 3: Making use of Artificial Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Artificial Neural Networks**: Compute the root mean squared error (% accuracy) between the predicted value and the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MODEL 3: Making use of Artificial Neural Networks (RMSE)\n",
    "# rms = sqrt(mean_squared_error(y_actual, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The most accurate model that can predict the happiness scores is the (____) model.\n",
    "\n",
    "The final model will thus be this model, and will be used to predict the happiness scores for the 2017 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting 2017's Happiness Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2017\n",
    "dataset2017 = pd.read_csv('dataset/2017.csv')\n",
    "X_2017 = dataset2017.iloc[:, :-1].values\n",
    "y_2017 = dataset2017.iloc[:, 8].values\n",
    "\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "X_2017[:, 0] = labelencoder.fit_transform(X_2017[:, 0])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X_2017 = onehotencoder.fit_transform(X_2017).toarray()\n",
    "\n",
    "# Avoiding the Dummy Variable Trap\n",
    "X_2017 = X_2017[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the 2017 dataset results using the model\n",
    "# y_pred = regressor.predict(X_2016_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
